# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, csunny
# This file is distributed under the same license as the DB-GPT package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: DB-GPT 👏👏 0.4.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-11-04 15:08+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../getting_started/install/deploy.rst:4 bb7e1a7b70624a3d91804c5ce2d5216b
msgid "Installation From Source"
msgstr "源码安装"

#: ../../getting_started/install/deploy.rst:6 7524477e8b8a456b8e2b942621a6b225
msgid "To get started, install DB-GPT with the following steps."
msgstr "按照以下步骤进行安装"

#: ../../getting_started/install/deploy.rst:10 871cdb0a7e004668b518168461d76945
msgid "1.Preparation"
msgstr "1.准备"

#: ../../getting_started/install/deploy.rst:11 d845f48ec05a4bb7a5b0f95a0644029d
msgid "**Download DB-GPT**"
msgstr "**下载DB-GPT项目**"

#: ../../getting_started/install/deploy.rst:17 85df7bb2b73141058ba0ca5a73e46701
msgid "**Install Miniconda**"
msgstr "**安装Miniconda**"

#: ../../getting_started/install/deploy.rst:19 b7b3e120ded344d09fb2d7a3a7903497
msgid ""
"We use Sqlite as default database, so there is no need for database "
"installation.  If you choose to connect to other databases, you can "
"follow our tutorial for installation and configuration. For the entire "
"installation process of DB-GPT, we use the miniconda3 virtual "
"environment. Create a virtual environment and install the Python "
"dependencies. `How to install Miniconda "
"<https://docs.conda.io/en/latest/miniconda.html>`_"
msgstr ""
"目前使用Sqlite作为默认数据库，因此DB-"
"GPT快速部署不需要部署相关数据库服务。如果你想使用其他数据库，需要先部署相关数据库服务。我们目前使用Miniconda进行python环境和包依赖管理[安装"
" Miniconda](https://docs.conda.io/en/latest/miniconda.html)"

#: ../../getting_started/install/deploy.rst:36 a5e34db6a6b8450c8a1f35377a1dc93b
msgid "2.Deploy LLM Service"
msgstr "2.部署LLM服务"

#: ../../getting_started/install/deploy.rst:37 5c49829b086e4d5fbcb5a971a34fa3b5
msgid ""
"DB-GPT can be deployed on servers with low hardware requirements or on "
"servers with high hardware requirements."
msgstr "DB-GPT可以部署在对硬件要求不高的服务器，也可以部署在对硬件要求高的服务器"

#: ../../getting_started/install/deploy.rst:39 7c2f7e2b49054d17850942bef9570c45
msgid ""
"If you are low hardware requirements you can install DB-GPT by Using "
"third-part LLM REST API Service OpenAI, Azure, tongyi."
msgstr "Low hardware requirements模式适用于对接第三方模型服务的api,比如OpenAI, 通义千问, 文心.cpp。"

#: ../../getting_started/install/deploy.rst:43 8c392e63073a47698e38456c5994ff25
msgid "As our project has the ability to achieve OpenAI performance of over 85%,"
msgstr "使用OpenAI服务可以让DB-GPT准确率达到85%"

#: ../../getting_started/install/deploy.rst:48 148f8d85c02345219ac535003fe05fca
msgid "Notice make sure you have install git-lfs"
msgstr "确认是否已经安装git-lfs"

#: ../../getting_started/install/deploy.rst:50 7f719273d1ff4d888376030531a560c4
msgid "centos:yum install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy.rst:52 f5c2035bca214e3fbef2b70be0e76247
msgid "ubuntu:apt-get install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy.rst:54 cb1b52b9323e4e9fbccfd3c42067a568
msgid "macos:brew install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy.rst:58
#: ../../getting_started/install/deploy.rst:223
#: 9c601ee0c06646d58cbd2c2fff342cb5 d3fe6c712b4145d0856487c65057c2a0
msgid "OpenAI"
msgstr "OpenAI"

#: ../../getting_started/install/deploy.rst:60
#: ../../getting_started/install/deploy.rst:207
#: 5e0e84bd11084d158e18b702022aaa96 6a9c9830e5184eac8b6b7425780f4a3e
msgid "Download embedding model"
msgstr "下载embedding model"

#: ../../getting_started/install/deploy.rst:72
#: ../../getting_started/install/deploy.rst:231
#: 953d6931627641b985588abf8b6ad9fd d52e287164ca44f79709f4645a1bb774
msgid "Configure LLM_MODEL and PROXY_API_URL and API_KEY in `.env` file"
msgstr "在`.env`文件设置LLM_MODEL and PROXY_API_URL and API_KEY"

#: ../../getting_started/install/deploy.rst:82
#: ../../getting_started/install/deploy.rst:282
#: 6dc89616c0bf46a898a49606d1349039 e30b7458fc0d46e8bcf0ec61a2be2e19
msgid "Make sure your .env configuration is not overwritten"
msgstr "认.env文件不会被覆盖\""

#: ../../getting_started/install/deploy.rst:85 7adce7ac878d4b1ab9249d840d302520
msgid "Vicuna"
msgstr "Vicuna"

#: ../../getting_started/install/deploy.rst:86 44a32c4cd0b140838b49f739d68ff42d
msgid ""
"`Vicuna-v1.5 <https://huggingface.co/lmsys/vicuna-13b-v1.5>`_ based on "
"llama-2 has been released, we recommend you set `LLM_MODEL=vicuna-"
"13b-v1.5` to try this model)"
msgstr ""

#: ../../getting_started/install/deploy.rst:88 f4991de79c434961b0ee777cf42dc491
msgid "vicuna-v1.5 hardware requirements"
msgstr ""

#: ../../getting_started/install/deploy.rst:92
#: ../../getting_started/install/deploy.rst:137
#: 596405ba21d046c993c18d0bcaf95048 ca993a5f1b194707bbe3d5d6e9d670d8
msgid "Model"
msgstr ""

#: ../../getting_started/install/deploy.rst:93
#: ../../getting_started/install/deploy.rst:138
#: 8437bb47daa94ca8a43b0eadc1286842 d040d964198d4c95afa6dd0d86586cc2
msgid "Quantize"
msgstr ""

#: ../../getting_started/install/deploy.rst:94
#: ../../getting_started/install/deploy.rst:139
#: 453bc4416d89421699d90d940498b9ac 5de996bb955f4c6ca8d5651985ced255
msgid "VRAM Size"
msgstr ""

#: ../../getting_started/install/deploy.rst:95
#: ../../getting_started/install/deploy.rst:98 077d52b8588e4ddd956acb7ff86bc458
#: fcf0ca75422343d19b12502fb7746f08
msgid "vicuna-7b-v1.5"
msgstr ""

#: ../../getting_started/install/deploy.rst:96
#: ../../getting_started/install/deploy.rst:102
#: ../../getting_started/install/deploy.rst:141
#: ../../getting_started/install/deploy.rst:147
#: 799b310b76454f79a261300253a9bb75 bd98df81ec8941b69c7d1de107154b1b
#: ca71731754f545d589e506cb53e6f042 d4023553e4384cc4955ef5bf4dc4d05b
msgid "4-bit"
msgstr ""

#: ../../getting_started/install/deploy.rst:97
#: ../../getting_started/install/deploy.rst:142
#: adfc22d9c6394046bb632831a4a3b066 e4e7762c714a49828da630a5f9d641dc
msgid "8 GB"
msgstr ""

#: ../../getting_started/install/deploy.rst:99
#: ../../getting_started/install/deploy.rst:105
#: ../../getting_started/install/deploy.rst:144
#: ../../getting_started/install/deploy.rst:150
#: 0d6bd1c4a6464df38723de978c09e8bc 106e20f7727f43d280b7560dc0f19bbc
#: 5a95691d1d134638bffa478daf2577a0 f8add03506244306b1ac6e10c5a2bc1f
msgid "8-bit"
msgstr ""

#: ../../getting_started/install/deploy.rst:100
#: ../../getting_started/install/deploy.rst:103
#: ../../getting_started/install/deploy.rst:145
#: ../../getting_started/install/deploy.rst:148
#: 03222e6a682e406bb56c21025412fbe5 2eb88af3efb74fb79899d7818a79d177
#: 9705b9d7d2084f0b8867f6aeec66251b bca6e5151e244a318f4c2078c3f82118
msgid "12 GB"
msgstr ""

#: ../../getting_started/install/deploy.rst:101
#: ../../getting_started/install/deploy.rst:104
#: 1596cbb77c954d1db00e2fa283d66a9a c6e8dc1600d14ee5a47f45f75ec709f4
msgid "vicuna-13b-v1.5"
msgstr ""

#: ../../getting_started/install/deploy.rst:106
#: ../../getting_started/install/deploy.rst:151
#: 00ee98593b7f4b29b5d97fa5a323bfa1 0169214dcf8d46c791b3482c81283fa0
msgid "20 GB"
msgstr ""

#: ../../getting_started/install/deploy.rst:122
#: ../../getting_started/install/deploy.rst:169
#: ../../getting_started/install/deploy.rst:195
#: 852f800e380c44d3b1b5bdaa881406b7 89ddd6c4a4874f4f8b7f051d797e364a
#: a2b4436f8fcb44ada3b5bf5c1099e646
msgid "The model files are large and will take a long time to download."
msgstr ""

#: ../../getting_started/install/deploy.rst:124
#: ../../getting_started/install/deploy.rst:171
#: ../../getting_started/install/deploy.rst:197
#: 18ccf7acdf3045588a19866c0161f44b 3d3d93649ec943c8b2b725807e969cbf
#: 6205975466f54d8fbf8e30b58df6b9c2
msgid "**Configure LLM_MODEL in `.env` file**"
msgstr ""

#: ../../getting_started/install/deploy.rst:131
#: ../../getting_started/install/deploy.rst:228
#: 022addaea4ea432584b8a27ea83a9dc4 7a95d25c58d2491e855dff42f6d3a686
msgid "Baichuan"
msgstr ""

#: ../../getting_started/install/deploy.rst:133
#: b5374f6d0ca44ee9bd984c0198515861
msgid "Baichuan hardware requirements"
msgstr ""

#: ../../getting_started/install/deploy.rst:140
#: ../../getting_started/install/deploy.rst:143
#: 6a5e6034c185455d81e8ba5349dbc191 9d0109817c524747909417924bb0bd31
msgid "baichuan-7b"
msgstr ""

#: ../../getting_started/install/deploy.rst:146
#: ../../getting_started/install/deploy.rst:149
#: b3300ed57f0c4ed98816652ea4a5c3c8 d57e98b46fb14f649d6ac8df27edcfd8
msgid "baichuan-13b"
msgstr ""

#: ../../getting_started/install/deploy.rst:173
#: ac684ccbea7d4f5380c43ad02a1b7231
msgid "please rename Baichuan path to \"baichuan2-13b\" or \"baichuan2-7b\""
msgstr "将Baichuan模型目录修改为\"baichuan2-13b\" 或 \"baichuan2-7b\""

#: ../../getting_started/install/deploy.rst:179
#: d2e283112ed1440b93510c331922b37c
msgid "ChatGLM"
msgstr ""

#: ../../getting_started/install/deploy.rst:199
#: ae30d065461e446e86b02d3903108a02
msgid "please rename chatglm model path to \"chatglm2-6b\""
msgstr "将chatglm模型目录修改为\"chatglm2-6b\""

#: ../../getting_started/install/deploy.rst:205
#: 81a8983e89c4400abc20080c86865c02
msgid "Other LLM API"
msgstr ""

#: ../../getting_started/install/deploy.rst:219
#: 3df3d5e0e4f64813a7ca2adccd056c4a
msgid "Now DB-GPT support LLM REST API TYPE:"
msgstr "目前DB-GPT支持的大模型REST API类型:"

#: ../../getting_started/install/deploy.rst:224
#: 4f7cb7a3f324410d8a34ba96d7c9536a
msgid "Azure"
msgstr ""

#: ../../getting_started/install/deploy.rst:225
#: ee6cbe48a8094637b6b8a049446042c6
msgid "Aliyun tongyi"
msgstr ""

#: ../../getting_started/install/deploy.rst:226
#: a3d9c061b0f440258be88cc3143b1887
msgid "Baidu wenxin"
msgstr ""

#: ../../getting_started/install/deploy.rst:227
#: 176001fb6eaf448f9eca1abd256b3ef4
msgid "Zhipu"
msgstr ""

#: ../../getting_started/install/deploy.rst:229
#: 17fecc140be2474ea1566d44641a27c0
msgid "Bard"
msgstr ""

#: ../../getting_started/install/deploy.rst:284
#: bc34832cec754984a40c4d01526f0f83
msgid "llama.cpp"
msgstr ""

#: ../../getting_started/install/deploy.rst:286
#: aa4b14e726dc447f92dec48f38a3d770
msgid ""
"DB-GPT already supports `llama.cpp "
"<https://github.com/ggerganov/llama.cpp>`_ via `llama-cpp-python "
"<https://github.com/abetlen/llama-cpp-python>`_ ."
msgstr ""
"DB-GPT 已经支持了 `llama.cpp <https://github.com/ggerganov/llama.cpp>`_ via "
"`llama-cpp-python <https://github.com/abetlen/llama-cpp-python>`_ ."

#: ../../getting_started/install/deploy.rst:288
#: e97a868ad05b48ee8d25017457c1b7ee
msgid "**Preparing Model Files**"
msgstr "**准备Model文件**"

#: ../../getting_started/install/deploy.rst:290
#: 8efc7824082545a4a98ebbd6b12fa00f
msgid ""
"To use llama.cpp, you need to prepare a gguf format model file, and there"
" are two common ways to obtain it, you can choose either:"
msgstr "使用 llama.cpp，你需要准备 gguf 格式的文件，你可以通过以下两种方法获取"

#: ../../getting_started/install/deploy.rst:292
#: 5b9f3a35779c4f89a4b9051904c5f7d8
msgid "**1. Download a pre-converted model file.**"
msgstr "**1.下载已转换的模型文件.**"

#: ../../getting_started/install/deploy.rst:294
#: e15f33a2e5fc4caaa9ad2e10cabf9d32
msgid ""
"Suppose you want to use [Vicuna 13B v1.5](https://huggingface.co/lmsys"
"/vicuna-13b-v1.5), you can download the file already converted from "
"[TheBloke/vicuna-13B-v1.5-GGUF](https://huggingface.co/TheBloke/vicuna-"
"13B-v1.5-GGUF), only one file is needed. Download it to the `models` "
"directory and rename it to `ggml-model-q4_0.gguf`."
msgstr ""
"假设您想使用[Vicuna 13B v1.5](https://huggingface.co/lmsys/vicuna-"
"13b-v1.5)您可以从[TheBloke/vicuna-"
"13B-v1.5-GGUF](https://huggingface.co/TheBloke/vicuna-"
"13B-v1.5-GGUF)下载已转换的文件，只需要一个文件。将其下载到models目录并将其重命名为 `ggml-"
"model-q4_0.gguf`。"

#: ../../getting_started/install/deploy.rst:300
#: 81ab603cd9b94cabb48add9541590b8b
msgid "**2. Convert It Yourself**"
msgstr "**2. 自行转换**"

#: ../../getting_started/install/deploy.rst:302
#: f9b46aa70a0842d6acc7e837c8b27778
msgid ""
"You can convert the model file yourself according to the instructions in "
"[llama.cpp#prepare-data--run](https://github.com/ggerganov/llama.cpp"
"#prepare-data--run), and put the converted file in the models directory "
"and rename it to `ggml-model-q4_0.gguf`."
msgstr ""
"您可以根据[llama.cpp#prepare-data--run](https://github.com/ggerganov/llama.cpp"
"#prepare-data--run)中的说明自行转换模型文件，并把转换后的文件放在models目录中，并重命名为`ggml-"
"model-q4_0.gguf`。"

#: ../../getting_started/install/deploy.rst:304
#: 5278c05568dd4ad688bcc181e45caa18
msgid "**Installing Dependencies**"
msgstr "**安装依赖**"

#: ../../getting_started/install/deploy.rst:306
#: bd67bf651a9a4096a37e73851b5fad98
msgid ""
"llama.cpp is an optional dependency in DB-GPT, and you can manually "
"install it using the following command:"
msgstr "llama.cpp在DB-GPT中是可选安装项, 你可以通过以下命令进行安装"

#: ../../getting_started/install/deploy.rst:313
#: 71ed6ae4d0aa4d3ca27ebfffc42a5095
msgid "**3.Modifying the Configuration File**"
msgstr "**3.修改配置文件**"

#: ../../getting_started/install/deploy.rst:315
#: a0a2fdb799524ef4938df2c306536e2a
msgid "Next, you can directly modify your `.env` file to enable llama.cpp."
msgstr "修改`.env`文件使用llama.cpp"

#: ../../getting_started/install/deploy.rst:322
#: ../../getting_started/install/deploy.rst:390
#: 03694cbb7dfa432cb32116925357f008
msgid ""
"Then you can run it according to `Run <https://db-"
"gpt.readthedocs.io/en/latest/getting_started/install/deploy/deploy.html#run>`_"
msgstr ""
"然后你可以根据[运行](https://db-gpt.readthedocs.io/projects/db-gpt-docs-zh-"
"cn/zh_CN/latest/getting_started/install/deploy/deploy.html#run)来运行"

#: ../../getting_started/install/deploy.rst:325
#: c9bbc15ea2fc4562bdf87a4e099600a0
msgid "**More Configurations**"
msgstr "**更多配置文件**"

#: ../../getting_started/install/deploy.rst:327
#: 9beff887d8554d1dac169de93625c1b2
msgid ""
"In DB-GPT, the model configuration can be done through  `{model "
"name}_{config key}`."
msgstr "在DB-GPT中，模型配置可以通过`{模型名称}_{配置名}` 来配置。"

#: ../../getting_started/install/deploy.rst:329
#: 6cad64bcc9f141de85479203b21f7f9e
msgid "More Configurations"
msgstr "**更多配置文件**"

#: ../../getting_started/install/deploy.rst:333
#: de97b0d5b59c4bfabbd47b7e1766cc70
msgid "Environment Variable Key"
msgstr "环境变量Key"

#: ../../getting_started/install/deploy.rst:334
#: 76afa3655f644576b54b4422bad3fcad
msgid "Default"
msgstr "默认值"

#: ../../getting_started/install/deploy.rst:335
#: 0dcabfeeb4874e08ba53d4b4aa6a1d73
msgid "Description"
msgstr "描述"

#: ../../getting_started/install/deploy.rst:336
#: c1670aec27cf4c068d825456817d7667
msgid "llama_cpp_prompt_template"
msgstr ""

#: ../../getting_started/install/deploy.rst:337
#: ../../getting_started/install/deploy.rst:340
#: ../../getting_started/install/deploy.rst:346
#: ../../getting_started/install/deploy.rst:352
#: ../../getting_started/install/deploy.rst:358
#: 49c8f875066f49c6aac195805f88e4a9
msgid "None"
msgstr ""

#: ../../getting_started/install/deploy.rst:338
#: bb31e19533e5434b832d5975bf579dc3
msgid ""
"Prompt template name, now support: zero_shot, vicuna_v1.1,alpaca,llama-2"
",baichuan-chat,internlm-chat, If None, the prompt template is "
"automatically determined from model path。"
msgstr ""
"Prompt template 现在可以支持`zero_shot, vicuna_v1.1,alpaca,llama-2,baichuan-"
"chat,internlm-chat`, 如果是None, 可以根据模型路径来自动获取模型 Prompt template"

#: ../../getting_started/install/deploy.rst:339
#: 38ed23e19e2a47cf8fbacb035cfe1292
msgid "llama_cpp_model_path"
msgstr ""

#: ../../getting_started/install/deploy.rst:341
#: eb069fdfdfeb4b17b723ee6733ba50c2
msgid "Model path"
msgstr "模型路径"

#: ../../getting_started/install/deploy.rst:342
#: 348d3ff3e4f44ceb99aadead11f5cca5
msgid "llama_cpp_n_gpu_layers"
msgstr ""

#: ../../getting_started/install/deploy.rst:343
#: 007deecf593b4553b6ca8df3e9240a28
msgid "1000000000"
msgstr ""

#: ../../getting_started/install/deploy.rst:344
#: 4da435c417f0482581895bf4d052a6a1
msgid ""
"Number of layers to offload to the GPU, Set this to 1000000000 to offload"
" all layers to the GPU. If your GPU VRAM is not enough, you can set a low"
" number, eg: 10"
msgstr "要将多少网络层转移到GPU上，将其设置为1000000000以将所有层转移到GPU上。如果您的 GPU 内存不足，可以设置较低的数字，例如：10。"

#: ../../getting_started/install/deploy.rst:345
#: 4f186505ea81467590cf817d116e6879
msgid "llama_cpp_n_threads"
msgstr ""

#: ../../getting_started/install/deploy.rst:347
#: b94dc2fb2cd14a4a8f38ba95b05fbb5b
msgid ""
"Number of threads to use. If None, the number of threads is automatically"
" determined"
msgstr "要使用的线程数量。如果为None，则线程数量将自动确定。"

#: ../../getting_started/install/deploy.rst:348
#: 867f2357430440eba4e749a8a39bff18
msgid "llama_cpp_n_batch"
msgstr ""

#: ../../getting_started/install/deploy.rst:349
#: 78516f7b23264147bae11e13426097eb
msgid "512"
msgstr ""

#: ../../getting_started/install/deploy.rst:350
#: c2ec97ffef3e4a70afef6634e78801a2
msgid "Maximum number of prompt tokens to batch together when calling llama_eval"
msgstr "在调用llama_eval时，批处理在一起的prompt tokens的最大数量"

#: ../../getting_started/install/deploy.rst:351
#: 29ad3c5ab7134da49d6fca3b42d734d6
msgid "llama_cpp_n_gqa"
msgstr ""

#: ../../getting_started/install/deploy.rst:353
#: e5bc322b0c824465b1adbd162838a3b7
msgid "Grouped-query attention. Must be 8 for llama-2 70b."
msgstr "对于 llama-2 70B 模型，Grouped-query attention 必须为8。"

#: ../../getting_started/install/deploy.rst:354
#: 43e7d412238a4f0ba8227938d9fa4172
msgid "llama_cpp_rms_norm_eps"
msgstr ""

#: ../../getting_started/install/deploy.rst:355
#: 6328db04645b4b089593291e2ca13f79
msgid "5e-06"
msgstr ""

#: ../../getting_started/install/deploy.rst:356
#: 6c8bf631cece42fa86954f5cf2d75503
msgid "5e-6 is a good value for llama-2 models."
msgstr "对于llama-2模型来说，5e-6是一个不错的值。"

#: ../../getting_started/install/deploy.rst:357
#: 22ebee39fe8b4cc18378447cac67e631
msgid "llama_cpp_cache_capacity"
msgstr ""

#: ../../getting_started/install/deploy.rst:359
#: 4dc98bede90f4237829f65513e4adf61
msgid "Maximum cache capacity. Examples: 2000MiB, 2GiB"
msgstr "模型缓存最大值. 例如: 2000MiB, 2GiB"

#: ../../getting_started/install/deploy.rst:360
#: bead617b5af943dab0bc1209823d3c22
msgid "llama_cpp_prefer_cpu"
msgstr ""

#: ../../getting_started/install/deploy.rst:361
#: 8efb44f1cfc54b1a8b3ca8ea138113ee
msgid "False"
msgstr ""

#: ../../getting_started/install/deploy.rst:362
#: b854e6a44c8348ceb72ac382b73ceec5
msgid ""
"If a GPU is available, it will be preferred by default, unless "
"prefer_cpu=False is configured."
msgstr "如果有可用的GPU，默认情况下会优先使用GPU，除非配置了 prefer_cpu=False。"

#: ../../getting_started/install/deploy.rst:365
#: 41b0440eb0074e5fa8c87dd404efe0ce
msgid "vllm"
msgstr ""

#: ../../getting_started/install/deploy.rst:367
#: 6fc445675bf74b2ea2fe0c7f2a64db69
msgid "vLLM is a fast and easy-to-use library for LLM inference and serving."
msgstr "\"vLLM 是一个快速且易于使用的 LLM 推理和服务的库。"

#: ../../getting_started/install/deploy.rst:369
#: aab5f913d29445c4b345180b8793ebc7
msgid "**Running vLLM**"
msgstr "**运行vLLM**"

#: ../../getting_started/install/deploy.rst:371
#: a9560d697821404cb0abdabf9f479645
msgid "**1.Installing Dependencies**"
msgstr "**1.安装依赖**"

#: ../../getting_started/install/deploy.rst:373
#: b008224d4f314e5c988335262c95a42e
msgid ""
"vLLM is an optional dependency in DB-GPT, and you can manually install it"
" using the following command:"
msgstr "vLLM 在 DB-GPT 是一个可选依赖, 你可以使用下面的命令手动安装它："

#: ../../getting_started/install/deploy.rst:379
#: 7dc0c8e996124177935a4e0d9ef19837
msgid "**2.Modifying the Configuration File**"
msgstr "**2.修改配置文件**"

#: ../../getting_started/install/deploy.rst:381
#: 49667576b44c46bf87d5bf4d207dd63a
msgid "Next, you can directly modify your .env file to enable vllm."
msgstr "你可以直接修改你的 `.env` 文件"

#: ../../getting_started/install/deploy.rst:388
#: f16a78f0ee6545babab0d66f12654c0a
msgid ""
"You can view the models supported by vLLM `here "
"<https://vllm.readthedocs.io/en/latest/models/supported_models.html"
"#supported-models>`_"
msgstr ""
"你可以在 "
"[这里](https://vllm.readthedocs.io/en/latest/models/supported_models.html"
"#supported-models) 查看 vLLM 支持的模型。"

#: ../../getting_started/install/deploy.rst:397
#: f7ae366723e9494d8177eeb963ba0ed9
msgid "3.Prepare sql example(Optional)"
msgstr "3.准备 sql example(可选)"

#: ../../getting_started/install/deploy.rst:398
#: d302aac8ddb346598fc9d73e0f6c2cbc
msgid "**(Optional) load examples into SQLite**"
msgstr "**(可选) load examples into SQLite**"

#: ../../getting_started/install/deploy.rst:405
#: ae2a315e40854f27a074f2c0f2506014
msgid "On windows platform:"
msgstr ""

#: ../../getting_started/install/deploy.rst:412
#: 36b4b7a813844af3a4ec8062b39059a3
msgid "4.Run db-gpt server"
msgstr "4.运行db-gpt server"

#: ../../getting_started/install/deploy.rst:418
#: e56f15e563484027b7efb2147c9b71a7
msgid "**Open http://localhost:5000 with your browser to see the product.**"
msgstr "打开浏览器访问http://localhost:5000"

#~ msgid ""
#~ "DB-GPT can be deployed on servers"
#~ " with low hardware requirements or on"
#~ " servers with high hardware requirements."
#~ " You can install DB-GPT by "
#~ "Using third-part LLM REST API "
#~ "Service OpenAI, Azure."
#~ msgstr ""

#~ msgid ""
#~ "And you can also install DB-GPT"
#~ " by deploy LLM Service by download"
#~ " LLM model."
#~ msgstr ""

